apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: vllm-benchmark-{{ include "jbenchmark.fullname" . }}
spec:
  eventBusName: {{ include "jbenchmark.fullname" . }}
  template:
    serviceAccountName: operate-workflow-sa
  dependencies:
    # ----- EVENT DEPENDENCY -----
    - name: benchmark
      eventSourceName: benchmark-{{ include "jbenchmark.fullname" . }}
      eventName: vllm
      # ----- EVENT RESOURCE TRANSFORM -----
      transform:
        script: |-
{{- include "jbenchmark.localTransform" . | nindent 10 }}

  # ----- SENSOR TRIGGER -----
  triggers:
    - template:
        name: webhook-workflow-trigger
        k8s:
          operation: create
          source:
            resource:

              # ----- START OF WORKFLOW -----
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                labels:
{{ include "jbenchmark.workflowLabels" . | toYaml | nindent 16 }}
                generateName: vllm-benchmark-
              spec:

{{- include "jbenchmark.workflowSpecs" . | nindent 16 }}

                # ----- WORKFLOW ARGUMENTS -----
                arguments:
                  parameters:

{{- $params := include "jbenchmark.extraLocalWorkflowParams" . | fromYaml }}
{{- include "jbenchmark.localWorkflowParams" . | nindent 20 }}
{{- $params.params | toYaml | nindent 20 }}

                # ----- ENTRYPOINT -----
                entrypoint: "dag"


                # ----- WORKFLOW TEMPLATES -----
                templates:

                  # ----- BACKGROUND MODEL TEMPLATE -----

                  # DO NOT CHANGE THE PLACEMENT OF THIS IN THE TEMPLATES LIST, THIS WILL FAIL THE RESOURCE INJECT
                  - name: "model-daemon"
                    daemon: true
                    # ----- TEMPLATE INPUTS -----
                    inputs:
                      parameters:
                        - name: experiment # experiment (json)
                      # artifacts:
                      #   - name: model-cache # model cache (files)
                      #     path: /root/.cache/huggingface
                    # ----- NODE SELECTOR & TOLERATIONS -----
                    nodeSelector:
                      jounce.io/nodetype: '{{`{{=fromJSON(inputs.parameters["experiment"]).machine.gpu_type }}`}}'
                    tolerations:
                      - key: "jounce.io/nodetype"
                        operator: "Equal"
                        value: '{{`{{=fromJSON(inputs.parameters["experiment"]).machine.gpu_type }}`}}'
                        effect: "NoExecute"
                      - key: "jounce.io/nodetype"
                        operator: "Equal"
                        value: '{{`{{=fromJSON(inputs.parameters["experiment"]).machine.gpu_type }}`}}'
                        effect: "NoSchedule"
                      - key: nvidia.com/gpu
                        effect: NoSchedule
                        value: "present"
                        operator: "Equal"
                    # ----- VOLUMES -----
                    volumes:
                      - name: "shared-memory-directory"
                        emptyDir:
                          medium": "Memory"
                    # ----- CONTAINER -----
                    container:
                      resources: {} # this will be overriden by sensor parameterization
                      readinessProbe: # readiness probe, equivalent to healthcheck in llmperf_wrapper
                        periodSeconds: 10
                        failureThreshold: 60
                        httpGet:
                          path: "/health"
                          port: 8000

                      env:
                        - name: VLLM_ATTENTION_BACKEND
                          value: '{{`{{=fromJSON(inputs.parameters["experiment"]).framework.vllm_attention_kernel ?? "" }}`}}'
                        - name: HF_TOKEN
                          valueFrom:
                            secretKeyRef:
                              name: huggingface-creds
                              key: HF_TOKEN

                      name: model
                      image: vllm/vllm-openai:0.6.3

                      command:
                        - "/bin/bash"
                        - "-c"

                      args:
                        - |
                          # remove empty environment variables
                          for var in $(env | awk -F= '{print $1}'); do
                            if [ -z "${!var}" ]; then
                              unset "$var"
                            fi
                          done

                          python3 -m vllm.entrypoints.openai.api_server \
                            --model {{`{{=fromJSON(inputs.parameters["experiment"]).model.hf_repo}}`}} \
                            --tensor-parallel-size {{`{{=fromJSON(inputs.parameters["experiment"]).framework.tensor_parallel_size}}`}} \
                            \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.skip_tokenizer_init ? "--skip-tokenizer-init" : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.enforce_eager ? "--enforce-eager" : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.disable_custom_all_reduce ? "--disable-custom-all-reduce" : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.enable_chunked_prefill ? "--enable-chunked-prefill" : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.enable_prefix_caching ? "--enable-prefix-caching" : "" }}`}} \
                            \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.tokenizer_mode != nil ? "--tokenizer-mode " + string(fromJSON(inputs.parameters["experiment"]).framework.tokenizer_mode) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.dtype != nil ? "--dtype " + string(fromJSON(inputs.parameters["experiment"]).framework.dtype) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.quantization != nil ? "--quantization " + string(fromJSON(inputs.parameters["experiment"]).framework.quantization) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.max_context_len_to_capture != nil ? "--max-context-len-to-capture " + string(fromJSON(inputs.parameters["experiment"]).framework.max_context_len_to_capture) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.max_num_seqs != nil ? "--max-num-seqs " + string(fromJSON(inputs.parameters["experiment"]).framework.max_num_seqs) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.max_seq_len_to_capture != nil ? "--max-seq-len-to-capture " + string(fromJSON(inputs.parameters["experiment"]).framework.max_seq_len_to_capture) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.max_model_len != nil ? "--max-model-len " + string(fromJSON(inputs.parameters["experiment"]).framework.max_model_len) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.kv_cache_dtype != nil ? "--kv-cache-dtype " + string(fromJSON(inputs.parameters["experiment"]).framework.kv_cache_dtype) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.max_num_batched_tokens != nil ? "--max-num-batched-tokens " + string(fromJSON(inputs.parameters["experiment"]).framework.max_num_batched_tokens) : "" }}`}} \
                            {{`{{=fromJSON(inputs.parameters["experiment"]).framework.gpu_memory_utilization != nil ? "--gpu-memory-utilization " + string(fromJSON(inputs.parameters["experiment"]).framework.gpu_memory_utilization) : "" }}`}} \
                            \
                            --trust-remote-code

                  # ----- DAG TEMPLATE -----
{{ include "jbenchmark.localDag" (dict
  "Values" .Values
  "Chart" .Chart
  "Release" .Release
  "Model" "{{=fromJSON(inputs.parameters[\"experiment\"]).model.hf_repo}}"
) | nindent 18 }}

          # ----- DEPENDENCY PARAMETERIZATION -----
          parameters:
            # RESOURCE INJECT - VOLATILE
            - src:
                dependencyName: benchmark
                # scenario should preferably be renamed to scenarios, personally I'm scared of the search_space codebase
                dataKey: body.resources
                useRawData: true
              dest: spec.templates.0.container.resources
            - src:
                dependencyName: benchmark
                dataKey: body.labels
                useRawData: true
              dest: spec.workflowMetadata.labels
            - src:
                dependencyName: benchmark
                dataKey: body.name
              dest: metadata.generateName
              operation: append

{{ include "jbenchmark.localDependencies" . | nindent 12 }}
